{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# MLX\n",
    "import mlx.core as mx\n",
    "import mlx.nn as mnn\n",
    "import mlx.optimizers as moptim\n",
    "from functools import partial\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Benchmark knobs\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 5e-4\n",
    "\n",
    "PRINT_EVERY = 200  # batches\n",
    "\n",
    "# MNIST normalization (common)\n",
    "MNIST_MEAN = np.array([0.1307], dtype=np.float32)\n",
    "MNIST_STD = np.array([0.3081], dtype=np.float32)\n",
    "\n",
    "def now():\n",
    "    return time.perf_counter()\n",
    "\n",
    "def torch_sync(device: str):\n",
    "    # For accurate timing; MPS is async.\n",
    "    if device == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "\n",
    "def mlx_sync(*arrays):\n",
    "    # MLX is lazy; force compute.\n",
    "    mx.eval(*arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 1, 28, 28) (60000,) | Test: (10000, 1, 28, 28) (10000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/_jtqhy053252zyb9r4y_h4yh0000gn/T/ipykernel_55741/2584586712.py:8: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  y = np.array(ds.targets, dtype=np.int64)\n"
     ]
    }
   ],
   "source": [
    "# Download once using torchvision, then store as uint8 in RAM.\n",
    "train_set = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True)\n",
    "test_set  = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True)\n",
    "\n",
    "def dataset_to_numpy(ds):\n",
    "    # MNIST: ds.data is uint8 [N, H, W]\n",
    "    x_nhw_u8 = ds.data.numpy()\n",
    "    y = np.array(ds.targets, dtype=np.int64)\n",
    "    return x_nhw_u8, y\n",
    "\n",
    "x_train_nhw_u8, y_train = dataset_to_numpy(train_set)\n",
    "x_test_nhw_u8,  y_test  = dataset_to_numpy(test_set)\n",
    "\n",
    "# Torch wants NCHW float32\n",
    "x_train_nchw = x_train_nhw_u8[:, None, :, :].astype(np.float32)  # N1HW\n",
    "x_test_nchw  = x_test_nhw_u8[:, None, :, :].astype(np.float32)\n",
    "\n",
    "# MLX wants NHWC float32\n",
    "x_train_nhwc = x_train_nhw_u8[:, :, :, None].astype(np.float32)  # NHW1\n",
    "x_test_nhwc  = x_test_nhw_u8[:, :, :, None].astype(np.float32)\n",
    "\n",
    "print(\"Train:\", x_train_nchw.shape, y_train.shape, \"| Test:\", x_test_nchw.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches_indices(n, batch_size, shuffle=True, seed=SEED):\n",
    "    idx = np.arange(n)\n",
    "    if shuffle:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(idx)\n",
    "    # drop_last\n",
    "    n_full = (n // batch_size) * batch_size\n",
    "    idx = idx[:n_full]\n",
    "    return idx.reshape(-1, batch_size)\n",
    "\n",
    "def iter_torch_batches(x_nchw_f32, y_i64, batch_size, device, shuffle=True, seed=SEED):\n",
    "    batches = make_batches_indices(len(y_i64), batch_size, shuffle=shuffle, seed=seed)\n",
    "    mean = torch.tensor(MNIST_MEAN.reshape(1, 1, 1, 1), device=device)\n",
    "    std  = torch.tensor(MNIST_STD.reshape(1, 1, 1, 1), device=device)\n",
    "\n",
    "    for bi, b in enumerate(batches):\n",
    "        xb = torch.from_numpy(x_nchw_f32[b]).to(device=device)\n",
    "        yb = torch.from_numpy(y_i64[b]).to(device=device)\n",
    "        xb = (xb / 255.0 - mean) / std\n",
    "        yield bi, xb, yb\n",
    "\n",
    "def iter_mlx_batches(x_nhwc_f32, y_i64, batch_size, shuffle=True, seed=SEED):\n",
    "    batches = make_batches_indices(len(y_i64), batch_size, shuffle=shuffle, seed=seed)\n",
    "    mean = mx.array(MNIST_MEAN.reshape(1, 1, 1, 1))\n",
    "    std  = mx.array(MNIST_STD.reshape(1, 1, 1, 1))\n",
    "\n",
    "    for bi, b in enumerate(batches):\n",
    "        xb = mx.array(x_nhwc_f32[b])\n",
    "        yb = mx.array(y_i64[b])\n",
    "        xb = (xb / 255.0 - mean) / std\n",
    "        yield bi, xb, yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.pool  = nn.MaxPool2d(kernel_size=2, stride=2)  # 28->14->7\n",
    "        self.fc1   = nn.Linear(64 * 7 * 7, 128, bias=True)\n",
    "        self.fc2   = nn.Linear(128, num_classes, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.flatten(1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def build_torch_simplecnn_mnist(num_classes=10):\n",
    "    return TorchSimpleCNN(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlxSimpleCNN(mnn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = mnn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv2 = mnn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.pool  = mnn.MaxPool2d(kernel_size=2, stride=2)  # 28->14->7\n",
    "        self.fc1   = mnn.Linear(64 * 7 * 7, 128, bias=True)\n",
    "        self.fc2   = mnn.Linear(128, num_classes, bias=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = mnn.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = mnn.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = mx.reshape(x, (x.shape[0], -1))\n",
    "        x = mnn.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## benchmark helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_torch_train(name: str, device: str, use_compile: bool):\n",
    "    assert device in (\"cpu\", \"mps\")\n",
    "    if device == \"mps\" and not torch.backends.mps.is_available():\n",
    "        return {\"name\": name, \"status\": \"SKIP (no MPS)\", \"device\": device}\n",
    "\n",
    "    torch_device = torch.device(device)\n",
    "    model = build_torch_simplecnn_mnist(num_classes=10).to(device=torch_device)\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # torch.compile (default backend is \"inductor\")  [oai_citation:4â€¡PyTorch Documentation](https://docs.pytorch.org/docs/stable/generated/torch.compile.html?utm_source=chatgpt.com)\n",
    "    if use_compile:\n",
    "        try:\n",
    "            model = torch.compile(model, backend=\"aot_eager\")\n",
    "        except Exception as e:\n",
    "            return {\"name\": name, \"status\": f\"FAIL compile: {type(e).__name__}: {e}\", \"device\": device}\n",
    "\n",
    "    # Warm-up single step (important esp. for compile + async backends)\n",
    "    model.train()\n",
    "    bi, xb, yb = next(iter_torch_batches(x_train_nchw, y_train, BATCH_SIZE, device, shuffle=True, seed=SEED))\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    out = model(xb)\n",
    "    loss = F.cross_entropy(out, yb)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    torch_sync(device)\n",
    "\n",
    "    epoch_times = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        t0 = now()\n",
    "        model.train()\n",
    "\n",
    "        for bi, xb, yb in iter_torch_batches(x_train_nchw, y_train, BATCH_SIZE, device, shuffle=True, seed=SEED+epoch+1):\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            out = model(xb)\n",
    "            loss = F.cross_entropy(out, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        torch_sync(device)\n",
    "        t1 = now()\n",
    "        epoch_times.append(t1 - t0)\n",
    "        print(f\"[{name}] epoch {epoch+1}/{EPOCHS} time: {epoch_times[-1]:.3f}s\")\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"status\": \"OK\",\n",
    "        \"device\": device,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"epoch_time_mean_s\": float(np.mean(epoch_times)),\n",
    "        \"epoch_time_std_s\": float(np.std(epoch_times)),\n",
    "        \"total_time_s\": float(np.sum(epoch_times)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mlx_train(name: str, use_compile: bool, *, sync_every: int = 1):\n",
    "    \"\"\"\n",
    "    MLX training loop optimized for correctness + performance.\n",
    "\n",
    "    - Correctness: forces model/optimizer state to be evaluated so updates actually happen.\n",
    "    - Performance: avoids recreating grad functions inside the compiled function, keeps batch shapes fixed,\n",
    "      and lets you control evaluation frequency via sync_every.\n",
    "    \"\"\"\n",
    "    assert sync_every >= 1, \"sync_every must be >= 1 to avoid building an unbounded lazy graph\"\n",
    "\n",
    "    model = MlxSimpleCNN(num_classes=10)\n",
    "    optimizer = moptim.SGD(\n",
    "        learning_rate=LR,\n",
    "        momentum=MOMENTUM,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "    def loss_fn(m, x, y):\n",
    "        logits = m(x)\n",
    "        return mx.mean(mnn.losses.cross_entropy(logits, y))\n",
    "\n",
    "    # Build the grad function ONCE (important for compile time + stability).\n",
    "    loss_and_grad_fn = mnn.value_and_grad(model, loss_fn)\n",
    "\n",
    "    # Track state that must be treated as inputs/outputs for compilation correctness.\n",
    "    # Keep mx.random.state only if you use randomness inside the step (dropout, stochastic aug, etc.).\n",
    "    state = [model.state, optimizer.state, mx.random.state]\n",
    "\n",
    "    def step_eager(x, y):\n",
    "        loss, grads = loss_and_grad_fn(model, x, y)\n",
    "        optimizer.update(model, grads)\n",
    "        return loss\n",
    "\n",
    "    @partial(mx.compile, inputs=state, outputs=state)\n",
    "    def step_compiled(x, y):\n",
    "        # No Python object creation in the hot path.\n",
    "        loss, grads = loss_and_grad_fn(model, x, y)\n",
    "        optimizer.update(model, grads)\n",
    "        return loss\n",
    "\n",
    "    step = step_compiled if use_compile else step_eager\n",
    "\n",
    "    # Warm-up (also triggers first-time compile if use_compile=True)\n",
    "    bi, xb, yb = next(iter_mlx_batches(x_train_nhwc, y_train, BATCH_SIZE, shuffle=True, seed=SEED))\n",
    "    loss = step(xb, yb)\n",
    "    # Force loss + state so the update is not optimized away.\n",
    "    mx.eval(loss, *state)\n",
    "\n",
    "    epoch_times = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        t0 = now()\n",
    "\n",
    "        for i, (bi, xb, yb) in enumerate(\n",
    "            iter_mlx_batches(x_train_nhwc, y_train, BATCH_SIZE, shuffle=True, seed=SEED + epoch + 1)\n",
    "        ):\n",
    "            loss = step(xb, yb)\n",
    "\n",
    "            # Correctness: force the update to execute.\n",
    "            # Performance: you can reduce how often you sync, but don't set it too high.\n",
    "            if (i + 1) % sync_every == 0:\n",
    "                mx.eval(loss, *state)\n",
    "\n",
    "        # Ensure the tail of the epoch is executed.\n",
    "        mx.eval(loss, *state)\n",
    "\n",
    "        t1 = now()\n",
    "        epoch_times.append(t1 - t0)\n",
    "        print(f\"[{name}] epoch {epoch+1}/{EPOCHS} time: {epoch_times[-1]:.3f}s\")\n",
    "\n",
    "    epoch_times = np.array(epoch_times, dtype=np.float64)\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"status\": \"OK\",\n",
    "        \"device\": \"mlx\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"use_compile\": bool(use_compile),\n",
    "        \"sync_every\": int(sync_every),\n",
    "        # Helpful to separate warmup/compile cost from steady state\n",
    "        \"epoch1_time_s\": float(epoch_times[0]) if len(epoch_times) else None,\n",
    "        \"epoch_time_mean_s\": float(epoch_times.mean()) if len(epoch_times) else None,\n",
    "        \"epoch_time_std_s\": float(epoch_times.std()) if len(epoch_times) else None,\n",
    "        \"epoch_time_mean_steady_s\": float(epoch_times[1:].mean()) if len(epoch_times) > 1 else float(epoch_times.mean()),\n",
    "        \"total_time_s\": float(epoch_times.sum()) if len(epoch_times) else None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch_cpu_baseline] epoch 1/20 time: 26.782s\n",
      "[torch_cpu_baseline] epoch 2/20 time: 47.867s\n",
      "[torch_cpu_baseline] epoch 3/20 time: 60.175s\n",
      "[torch_cpu_baseline] epoch 4/20 time: 54.318s\n",
      "[torch_cpu_baseline] epoch 5/20 time: 21.071s\n",
      "[torch_cpu_baseline] epoch 6/20 time: 21.543s\n",
      "[torch_cpu_baseline] epoch 7/20 time: 19.541s\n",
      "[torch_cpu_baseline] epoch 8/20 time: 19.579s\n",
      "[torch_cpu_baseline] epoch 9/20 time: 19.582s\n",
      "[torch_cpu_baseline] epoch 10/20 time: 19.521s\n",
      "[torch_cpu_baseline] epoch 11/20 time: 19.599s\n",
      "[torch_cpu_baseline] epoch 12/20 time: 19.887s\n",
      "[torch_cpu_baseline] epoch 13/20 time: 19.669s\n",
      "[torch_cpu_baseline] epoch 14/20 time: 19.512s\n",
      "[torch_cpu_baseline] epoch 15/20 time: 19.584s\n",
      "[torch_cpu_baseline] epoch 16/20 time: 19.418s\n",
      "[torch_cpu_baseline] epoch 17/20 time: 19.683s\n",
      "[torch_cpu_baseline] epoch 18/20 time: 19.432s\n",
      "[torch_cpu_baseline] epoch 19/20 time: 19.699s\n",
      "[torch_cpu_baseline] epoch 20/20 time: 19.584s\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "results.append(run_torch_train(\"torch_cpu_baseline\", device=\"cpu\", use_compile=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch_cpu_compile] epoch 1/20 time: 19.365s\n",
      "[torch_cpu_compile] epoch 2/20 time: 19.286s\n",
      "[torch_cpu_compile] epoch 3/20 time: 19.367s\n",
      "[torch_cpu_compile] epoch 4/20 time: 19.361s\n",
      "[torch_cpu_compile] epoch 5/20 time: 19.624s\n",
      "[torch_cpu_compile] epoch 6/20 time: 19.291s\n",
      "[torch_cpu_compile] epoch 7/20 time: 19.455s\n",
      "[torch_cpu_compile] epoch 8/20 time: 19.069s\n",
      "[torch_cpu_compile] epoch 9/20 time: 19.235s\n",
      "[torch_cpu_compile] epoch 10/20 time: 19.838s\n",
      "[torch_cpu_compile] epoch 11/20 time: 19.314s\n",
      "[torch_cpu_compile] epoch 12/20 time: 19.235s\n",
      "[torch_cpu_compile] epoch 13/20 time: 19.529s\n",
      "[torch_cpu_compile] epoch 14/20 time: 19.025s\n",
      "[torch_cpu_compile] epoch 15/20 time: 18.865s\n",
      "[torch_cpu_compile] epoch 16/20 time: 19.262s\n",
      "[torch_cpu_compile] epoch 17/20 time: 19.598s\n",
      "[torch_cpu_compile] epoch 18/20 time: 19.251s\n",
      "[torch_cpu_compile] epoch 19/20 time: 19.304s\n",
      "[torch_cpu_compile] epoch 20/20 time: 19.259s\n"
     ]
    }
   ],
   "source": [
    "results.append(run_torch_train(\"torch_cpu_compile\", device=\"cpu\", use_compile=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch_mps] epoch 1/20 time: 3.863s\n",
      "[torch_mps] epoch 2/20 time: 3.841s\n",
      "[torch_mps] epoch 3/20 time: 3.883s\n",
      "[torch_mps] epoch 4/20 time: 3.865s\n",
      "[torch_mps] epoch 5/20 time: 4.359s\n",
      "[torch_mps] epoch 6/20 time: 3.824s\n",
      "[torch_mps] epoch 7/20 time: 3.870s\n",
      "[torch_mps] epoch 8/20 time: 3.833s\n",
      "[torch_mps] epoch 9/20 time: 3.823s\n",
      "[torch_mps] epoch 10/20 time: 3.821s\n",
      "[torch_mps] epoch 11/20 time: 3.833s\n",
      "[torch_mps] epoch 12/20 time: 3.890s\n",
      "[torch_mps] epoch 13/20 time: 3.826s\n",
      "[torch_mps] epoch 14/20 time: 3.835s\n",
      "[torch_mps] epoch 15/20 time: 3.858s\n",
      "[torch_mps] epoch 16/20 time: 3.844s\n",
      "[torch_mps] epoch 17/20 time: 3.873s\n",
      "[torch_mps] epoch 18/20 time: 4.449s\n",
      "[torch_mps] epoch 19/20 time: 4.113s\n",
      "[torch_mps] epoch 20/20 time: 4.074s\n"
     ]
    }
   ],
   "source": [
    "results.append(run_torch_train(\"torch_mps\", device=\"mps\", use_compile=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch_mps_compile] epoch 1/20 time: 4.087s\n",
      "[torch_mps_compile] epoch 2/20 time: 3.954s\n",
      "[torch_mps_compile] epoch 3/20 time: 3.986s\n",
      "[torch_mps_compile] epoch 4/20 time: 4.135s\n",
      "[torch_mps_compile] epoch 5/20 time: 4.014s\n",
      "[torch_mps_compile] epoch 6/20 time: 4.040s\n",
      "[torch_mps_compile] epoch 7/20 time: 4.016s\n",
      "[torch_mps_compile] epoch 8/20 time: 4.058s\n",
      "[torch_mps_compile] epoch 9/20 time: 4.062s\n",
      "[torch_mps_compile] epoch 10/20 time: 3.988s\n",
      "[torch_mps_compile] epoch 11/20 time: 3.964s\n",
      "[torch_mps_compile] epoch 12/20 time: 3.983s\n",
      "[torch_mps_compile] epoch 13/20 time: 4.053s\n",
      "[torch_mps_compile] epoch 14/20 time: 4.043s\n",
      "[torch_mps_compile] epoch 15/20 time: 3.961s\n",
      "[torch_mps_compile] epoch 16/20 time: 3.955s\n",
      "[torch_mps_compile] epoch 17/20 time: 3.944s\n",
      "[torch_mps_compile] epoch 18/20 time: 3.969s\n",
      "[torch_mps_compile] epoch 19/20 time: 3.979s\n",
      "[torch_mps_compile] epoch 20/20 time: 4.099s\n"
     ]
    }
   ],
   "source": [
    "results.append(run_torch_train(\"torch_mps_compile\", device=\"mps\", use_compile=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlx] epoch 1/20 time: 6.890s\n",
      "[mlx] epoch 2/20 time: 6.920s\n",
      "[mlx] epoch 3/20 time: 6.884s\n",
      "[mlx] epoch 4/20 time: 6.893s\n",
      "[mlx] epoch 5/20 time: 6.876s\n",
      "[mlx] epoch 6/20 time: 7.326s\n",
      "[mlx] epoch 7/20 time: 7.042s\n",
      "[mlx] epoch 8/20 time: 6.869s\n",
      "[mlx] epoch 9/20 time: 6.853s\n",
      "[mlx] epoch 10/20 time: 6.892s\n",
      "[mlx] epoch 11/20 time: 6.880s\n",
      "[mlx] epoch 12/20 time: 6.868s\n",
      "[mlx] epoch 13/20 time: 6.867s\n",
      "[mlx] epoch 14/20 time: 6.861s\n",
      "[mlx] epoch 15/20 time: 6.871s\n",
      "[mlx] epoch 16/20 time: 6.913s\n",
      "[mlx] epoch 17/20 time: 6.931s\n",
      "[mlx] epoch 18/20 time: 6.918s\n",
      "[mlx] epoch 19/20 time: 6.948s\n",
      "[mlx] epoch 20/20 time: 6.912s\n"
     ]
    }
   ],
   "source": [
    "results.append(run_mlx_train(\"mlx\", use_compile=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlx_compile] epoch 1/20 time: 5.809s\n",
      "[mlx_compile] epoch 2/20 time: 5.801s\n",
      "[mlx_compile] epoch 3/20 time: 5.825s\n",
      "[mlx_compile] epoch 4/20 time: 5.788s\n",
      "[mlx_compile] epoch 5/20 time: 5.779s\n",
      "[mlx_compile] epoch 6/20 time: 5.784s\n",
      "[mlx_compile] epoch 7/20 time: 5.774s\n",
      "[mlx_compile] epoch 8/20 time: 5.798s\n",
      "[mlx_compile] epoch 9/20 time: 5.784s\n",
      "[mlx_compile] epoch 10/20 time: 5.786s\n",
      "[mlx_compile] epoch 11/20 time: 5.789s\n",
      "[mlx_compile] epoch 12/20 time: 5.786s\n",
      "[mlx_compile] epoch 13/20 time: 5.821s\n",
      "[mlx_compile] epoch 14/20 time: 5.801s\n",
      "[mlx_compile] epoch 15/20 time: 5.799s\n",
      "[mlx_compile] epoch 16/20 time: 5.785s\n",
      "[mlx_compile] epoch 17/20 time: 5.785s\n",
      "[mlx_compile] epoch 18/20 time: 5.840s\n",
      "[mlx_compile] epoch 19/20 time: 5.824s\n",
      "[mlx_compile] epoch 20/20 time: 5.826s\n"
     ]
    }
   ],
   "source": [
    "results.append(run_mlx_train(\"mlx_compile\", use_compile=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Raw results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>status</th>\n",
       "      <th>device</th>\n",
       "      <th>epochs</th>\n",
       "      <th>epoch_time_mean_s</th>\n",
       "      <th>epoch_time_std_s</th>\n",
       "      <th>total_time_s</th>\n",
       "      <th>use_compile</th>\n",
       "      <th>sync_every</th>\n",
       "      <th>epoch1_time_s</th>\n",
       "      <th>epoch_time_mean_steady_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>torch_cpu_baseline</td>\n",
       "      <td>OK</td>\n",
       "      <td>cpu</td>\n",
       "      <td>20</td>\n",
       "      <td>25.302321</td>\n",
       "      <td>12.365466</td>\n",
       "      <td>506.046421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>torch_cpu_compile</td>\n",
       "      <td>OK</td>\n",
       "      <td>cpu</td>\n",
       "      <td>20</td>\n",
       "      <td>19.326674</td>\n",
       "      <td>0.211065</td>\n",
       "      <td>386.533488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>torch_mps</td>\n",
       "      <td>OK</td>\n",
       "      <td>mps</td>\n",
       "      <td>20</td>\n",
       "      <td>3.864911</td>\n",
       "      <td>0.015598</td>\n",
       "      <td>77.298214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>torch_mps_compile</td>\n",
       "      <td>OK</td>\n",
       "      <td>mps</td>\n",
       "      <td>20</td>\n",
       "      <td>4.014458</td>\n",
       "      <td>0.053598</td>\n",
       "      <td>80.289153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mlx</td>\n",
       "      <td>OK</td>\n",
       "      <td>mlx</td>\n",
       "      <td>20</td>\n",
       "      <td>6.920760</td>\n",
       "      <td>0.101687</td>\n",
       "      <td>138.415210</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.890366</td>\n",
       "      <td>6.922360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mlx_compile</td>\n",
       "      <td>OK</td>\n",
       "      <td>mlx</td>\n",
       "      <td>20</td>\n",
       "      <td>5.799211</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>115.984212</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.809232</td>\n",
       "      <td>5.798683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name status device  epochs  epoch_time_mean_s  \\\n",
       "0  torch_cpu_baseline     OK    cpu      20          25.302321   \n",
       "1   torch_cpu_compile     OK    cpu      20          19.326674   \n",
       "2           torch_mps     OK    mps      20           3.864911   \n",
       "3   torch_mps_compile     OK    mps      20           4.014458   \n",
       "4                 mlx     OK    mlx      20           6.920760   \n",
       "5         mlx_compile     OK    mlx      20           5.799211   \n",
       "\n",
       "   epoch_time_std_s  total_time_s use_compile  sync_every  epoch1_time_s  \\\n",
       "0         12.365466    506.046421         NaN         NaN            NaN   \n",
       "1          0.211065    386.533488         NaN         NaN            NaN   \n",
       "2          0.015598     77.298214         NaN         NaN            NaN   \n",
       "3          0.053598     80.289153         NaN         NaN            NaN   \n",
       "4          0.101687    138.415210       False         1.0       6.890366   \n",
       "5          0.018315    115.984212        True         1.0       5.809232   \n",
       "\n",
       "   epoch_time_mean_steady_s  \n",
       "0                       NaN  \n",
       "1                       NaN  \n",
       "2                       NaN  \n",
       "3                       NaN  \n",
       "4                  6.922360  \n",
       "5                  5.798683  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ranking (fastest mean epoch time first) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>device</th>\n",
       "      <th>epoch_time_mean_s</th>\n",
       "      <th>epoch_time_std_s</th>\n",
       "      <th>total_time_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>torch_mps</td>\n",
       "      <td>mps</td>\n",
       "      <td>3.864911</td>\n",
       "      <td>0.015598</td>\n",
       "      <td>77.298214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>torch_mps_compile</td>\n",
       "      <td>mps</td>\n",
       "      <td>4.014458</td>\n",
       "      <td>0.053598</td>\n",
       "      <td>80.289153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>mlx_compile</td>\n",
       "      <td>mlx</td>\n",
       "      <td>5.799211</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>115.984212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mlx</td>\n",
       "      <td>mlx</td>\n",
       "      <td>6.920760</td>\n",
       "      <td>0.101687</td>\n",
       "      <td>138.415210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>torch_cpu_compile</td>\n",
       "      <td>cpu</td>\n",
       "      <td>19.326674</td>\n",
       "      <td>0.211065</td>\n",
       "      <td>386.533488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>torch_cpu_baseline</td>\n",
       "      <td>cpu</td>\n",
       "      <td>25.302321</td>\n",
       "      <td>12.365466</td>\n",
       "      <td>506.046421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name device  epoch_time_mean_s  epoch_time_std_s  \\\n",
       "2           torch_mps    mps           3.864911          0.015598   \n",
       "3   torch_mps_compile    mps           4.014458          0.053598   \n",
       "5         mlx_compile    mlx           5.799211          0.018315   \n",
       "4                 mlx    mlx           6.920760          0.101687   \n",
       "1   torch_cpu_compile    cpu          19.326674          0.211065   \n",
       "0  torch_cpu_baseline    cpu          25.302321         12.365466   \n",
       "\n",
       "   total_time_s  \n",
       "2     77.298214  \n",
       "3     80.289153  \n",
       "5    115.984212  \n",
       "4    138.415210  \n",
       "1    386.533488  \n",
       "0    506.046421  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Keep rows that have timing\n",
    "timed = df[df[\"status\"].eq(\"OK\")].copy()\n",
    "timed = timed.sort_values(\"epoch_time_mean_s\", ascending=True)\n",
    "\n",
    "print(\"=== Raw results ===\")\n",
    "display(df)\n",
    "\n",
    "print(\"\\n=== Ranking (fastest mean epoch time first) ===\")\n",
    "display(timed[[\"name\",\"device\",\"epoch_time_mean_s\",\"epoch_time_std_s\",\"total_time_s\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
